{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Qwen 2.5-vl-32b**"
      ],
      "metadata": {
        "id": "ld2tPVNJJl-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Commonsense"
      ],
      "metadata": {
        "id": "tlRJeyyXMqrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "H_LVagGPJwtv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D_oLHj6b8w3",
        "outputId": "5b1fc570-e554-4f67-c031-2b2f868c1b8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7, 0.7058823529411765, 0.75, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/cm_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid1/prediction_cm_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot Prompting"
      ],
      "metadata": {
        "id": "yXfXIv4wJyqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/cm_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid2/prediction_cm_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "id": "jbQBdQpTcLxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "220611d3-b0aa-4bed-c99c-da363b8721b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.72, 0.7307692307692307, 0.76, 0.7037037037037037)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role Based Prompting"
      ],
      "metadata": {
        "id": "LHrrnYUWJ053"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/cm_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid3/prediction_cm_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV3sEFD0MkOC",
        "outputId": "0edb27bc-7009-4dd8-e1ea-0ea67444fb34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.74, 0.7450980392156863, 0.7916666666666666, 0.7037037037037037)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deonotology"
      ],
      "metadata": {
        "id": "EFy3aLrAMwqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "tWQjYJPUJ_WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/deontology_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid1/prediction_deontology_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHzo_SFBNJ08",
        "outputId": "8301b8be-0d3f-4497-ae57-8bb264cfa684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.92, 0.92, 0.92, 0.92)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot Prompting"
      ],
      "metadata": {
        "id": "fCD0HP6WKBTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/deontology_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid2/prediction_deontology_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Oc-H0m5Nu8Y",
        "outputId": "b9f824b8-5d97-48c5-c67b-eb730adb8443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.86, 0.8627450980392157, 0.8461538461538461, 0.88)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role-Based Prompting"
      ],
      "metadata": {
        "id": "p_kyj5LwKDS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/deontology_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid3/prediction_deontology_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfvyMszrNzUu",
        "outputId": "dd65dc08-c475-49bb-b9c8-914683e8b30b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.86, 0.8571428571428571, 0.875, 0.84)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Justice"
      ],
      "metadata": {
        "id": "iJaid90vN67H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "N8uqWANtKKHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/justice_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid1/prediction_justice_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBJxfm7QN8jE",
        "outputId": "f136103b-0d93-4ec7-ee94-a68a2d616746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.82, 0.8085106382978723, 0.7916666666666666, 0.8260869565217391)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot Prompting"
      ],
      "metadata": {
        "id": "n7mlIMIZKNkw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/justice_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid2/prediction_justice_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9x_YNAxOXYF",
        "outputId": "aef51fb4-f7e5-4c09-fadf-19d64a5c93f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.8, 0.7407407407407407, 0.8695652173913043)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role-Based Prompting"
      ],
      "metadata": {
        "id": "O2J2xRt9KLXX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/justice_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid3/prediction_justice_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XJoiy7xOgXH",
        "outputId": "9f6f43ee-eeda-41f7-d1f9-2aaffd574003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.8, 0.7407407407407407, 0.8695652173913043)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Virtue"
      ],
      "metadata": {
        "id": "StoFXbPoPFkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "ItIQGfllKY6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/virtue_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid1/prediction_virtue_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF81a72VPIly",
        "outputId": "a87791cb-b1ff-4b46-aee2-aef39a2a5bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.88, 0.75, 0.6, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot Prompting"
      ],
      "metadata": {
        "id": "T7M3K8sBKa-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/virtue_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid2/prediction_virtue_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtn3sFJPp6Q",
        "outputId": "95cf1f47-fc2f-4e28-c974-10d2c2d576bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9, 0.782608695652174, 0.6428571428571429, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role-based Prompting"
      ],
      "metadata": {
        "id": "URUffUxcKc41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"./true/virtue_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"./promptid3/prediction_virtue_test_50.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "id": "uVoRUfC3QLpY",
        "outputId": "bec924c4-2665-4a9e-adb1-0afc8c2317c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.86, 0.6956521739130435, 0.5714285714285714, 0.8888888888888888)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwsJc79B0AvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}