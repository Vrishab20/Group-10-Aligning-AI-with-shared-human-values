{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Commonsense**"
      ],
      "metadata": {
        "id": "tlRJeyyXMqrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "Q6rLi56_Bmnh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8D_oLHj6b8w3",
        "outputId": "d752ffcd-46d2-4271-d323-d74666416860"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5, 0.46808510638297873, 0.55, 0.4074074074074074)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/cm_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/CS1.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['Label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot Prompting"
      ],
      "metadata": {
        "id": "psO5meC9Bq28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/cm_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/CS2.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "id": "jbQBdQpTcLxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af545aff-b434-4497-84c4-70feb0c38bf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8, 0.782608695652174, 0.9473684210526315, 0.6666666666666666)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role-based Prompting"
      ],
      "metadata": {
        "id": "YhfkIK_RByXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/cm_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/CS3.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV3sEFD0MkOC",
        "outputId": "b326b1b1-267d-4965-8b89-82ea490a9749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.54, 0.5490196078431373, 0.5833333333333334, 0.5185185185185185)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Deonotology**"
      ],
      "metadata": {
        "id": "EFy3aLrAMwqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "lnegFBHeB-Qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/deontology_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/D1.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHzo_SFBNJ08",
        "outputId": "9f4df58f-f1c5-42d3-8030-e4334aadac89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52, 0.6129032258064516, 0.5135135135135135, 0.76)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot Prompting"
      ],
      "metadata": {
        "id": "2mZgJl4MB-hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/deontology_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/D2.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Oc-H0m5Nu8Y",
        "outputId": "0c88060b-d65a-4027-bc82-d4767fdb58f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.76, 0.7857142857142857, 0.7096774193548387, 0.88)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role-based Prompting"
      ],
      "metadata": {
        "id": "eDJq3ZbIB-p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/deontology_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/D3.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfvyMszrNzUu",
        "outputId": "a5d7acdd-db42-4736-c37e-b58bbbf1678a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.84, 0.8461538461538461, 0.8148148148148148, 0.88)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Justice**"
      ],
      "metadata": {
        "id": "iJaid90vN67H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "3EThFw1YCPfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/justice_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/J1.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBJxfm7QN8jE",
        "outputId": "60f1a0dd-ab7b-4ac1-cd6a-bfdff4eebba8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.54, 0.4888888888888889, 0.5, 0.4782608695652174)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot Prompting"
      ],
      "metadata": {
        "id": "37eBdYQKCPya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/justice_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/J2.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9x_YNAxOXYF",
        "outputId": "5ee06e9d-57a5-400b-ecae-517a9519aa03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.58, 0.6666666666666666, 0.525, 0.9130434782608695)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role-based Prompting"
      ],
      "metadata": {
        "id": "ITUm1g_-CP8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/justice_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/J3.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XJoiy7xOgXH",
        "outputId": "ffd594a1-6ba3-4f91-d85b-6c480e0af0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.56, 0.6071428571428571, 0.5151515151515151, 0.7391304347826086)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Virtue**"
      ],
      "metadata": {
        "id": "StoFXbPoPFkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline"
      ],
      "metadata": {
        "id": "livAIUNmCiQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/virtue_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/V1.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nF81a72VPIly",
        "outputId": "450164b6-e2ad-41b1-a954-5f1b7e9f2c35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.52, 0.3684210526315789, 0.2413793103448276, 0.7777777777777778)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Few-shot prompting"
      ],
      "metadata": {
        "id": "WbtoX8_vCieL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/virtue_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/V2.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtn3sFJPp6Q",
        "outputId": "b54eae7a-1451-4d24-a4e1-b4125f6de0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.46, 0.18181818181818182, 0.125, 0.3333333333333333)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Role-based Prompting"
      ],
      "metadata": {
        "id": "CSEY0_kGCinF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Load the two test CSV files: one with true labels and one with predicted labels\n",
        "true_labels_df = pd.read_csv(\"/content/virtue_test_50.csv\")  # original file with true labels\n",
        "predicted_labels_df = pd.read_csv(\"/content/V3.csv\")  # updated file with predictions\n",
        "\n",
        "# Extract labels\n",
        "y_true = true_labels_df['label']\n",
        "y_pred = predicted_labels_df['label']\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "\n",
        "accuracy, f1, precision, recall"
      ],
      "metadata": {
        "id": "uVoRUfC3QLpY",
        "outputId": "9c1cd00e-124a-4259-e16a-c14c3ed18572",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.78, 0.56, 0.4375, 0.7777777777777778)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EwsJc79B0AvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}